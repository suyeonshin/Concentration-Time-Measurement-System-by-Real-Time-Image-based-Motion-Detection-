{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abf0f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import os.path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Union, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab3257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import os\n",
    "import glob\n",
    "from os.path import normpath, basename\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def check_mkdir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        \n",
    "def create_frames_from_video(video_location, save_folder ,name_prefix='img', extension='jpg'):\n",
    "    # Read the video from specified path\n",
    "    cam = cv2.VideoCapture(video_location)\n",
    "    currentframe = 1\n",
    "    while(True):\n",
    "\n",
    "        # reading from frame\n",
    "        ret,frame = cam.read()\n",
    "        if ret:\n",
    "            # if video is still left continue creating images\n",
    "            name= os.path.join(save_folder, f'{name_prefix}_{currentframe:05d}.{extension}')\n",
    "\n",
    "            # writing the extracted images\n",
    "            cv2.imwrite(name, frame)\n",
    "\n",
    "            # increasing counter so that it will\n",
    "            # show how many frames are created\n",
    "            currentframe += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return currentframe\n",
    "\n",
    "def plot_video(rows, cols, frame_list, plot_width, plot_height, title: str):\n",
    "    fig = plt.figure(figsize=(plot_width, plot_height))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(rows, cols),  # creates 2x2 grid of axes\n",
    "                     axes_pad=0.3,  # pad between axes in inch.\n",
    "                     )\n",
    "\n",
    "    for index, (ax, im) in enumerate(zip(grid, frame_list)):\n",
    "        # Iterating over the grid returns the Axes.\n",
    "        ax.imshow(im)\n",
    "        ax.set_title(index)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "    \n",
    "def denormalize(video_tensor):\n",
    "    \"\"\"\n",
    "    Undoes mean/standard deviation normalization, zero to one scaling,\n",
    "    and channel rearrangement for a batch of images.\n",
    "    args:\n",
    "        video_tensor: a (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "    \"\"\"\n",
    "    inverse_normalize = transforms.Normalize(\n",
    "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
    "    )\n",
    "    return (inverse_normalize(video_tensor) * 255.).type(torch.uint8).permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "class ShiftWithChannelTensor:\n",
    "    def __call__(self, data):\n",
    "        return data.permute(1, 0, 2, 3).contiguous()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f307d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MobilenetV2 in PyTorch.\n",
    "See the paper \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" for more details.\n",
    "'''\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(inp, oup, kernel_size=3, stride=stride, padding=(1,1,1), bias=False),\n",
    "        nn.BatchNorm3d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm3d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == (1,1,1) and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv3d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv3d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm3d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv3d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv3d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv3d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm3d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, sample_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1,  16, 1, (1,1,1)],\n",
    "            [6,  24, 2, (2,2,2)],\n",
    "            [6,  32, 3, (2,2,2)],\n",
    "            [6,  64, 4, (2,2,2)],\n",
    "            [6,  96, 3, (1,1,1)],\n",
    "            [6, 160, 3, (2,2,2)],\n",
    "            [6, 320, 1, (1,1,1)],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert sample_size % 16 == 0.\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, (1,2,2))]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else (1,1,1)\n",
    "                self.features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.avg_pool3d(x, x.data.size()[-3:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941b8b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImglistToTensor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Converts a list of PIL images in the range [0,255] to a torch.FloatTensor\n",
    "    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1].\n",
    "    Can be used as first transform for ``VideoFrameDataset``.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(img_list: List[Image.Image]) -> 'torch.Tensor[NUM_IMAGES, CHANNELS, HEIGHT, WIDTH]':\n",
    "        \"\"\"\n",
    "        Converts each PIL image in a list to\n",
    "        a torch Tensor and stacks them into\n",
    "        a single tensor.\n",
    "\n",
    "        Args:\n",
    "            img_list: list of PIL images.\n",
    "        Returns:\n",
    "            tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        # print(type(img_list))\n",
    "        # print(img_list)\n",
    "        # print(np.array(img_list).shape)\n",
    "        return torch.stack([transforms.functional.to_tensor(pic) for pic in img_list])\n",
    "        # return torch.stack([transforms.functional.to_tensor(img_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e0da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from typing import List, Union, Tuple, Any\n",
    "\n",
    "\n",
    "class VideoRecord(object):\n",
    "    \"\"\"\n",
    "    Helper class for class VideoFrameDataset. This class\n",
    "    represents a video sample's metadata.\n",
    "\n",
    "    Args:\n",
    "        root_datapath: the system path to the root folder\n",
    "                       of the videos.\n",
    "        row: A list with four or more elements where 1) The first\n",
    "             element is the path to the video sample's frames excluding\n",
    "             the root_datapath prefix 2) The  second element is the starting frame id of the video\n",
    "             3) The third element is the inclusive ending frame id of the video\n",
    "             4) The fourth element is the label index.\n",
    "             5) any following elements are labels in the case of multi-label classification\n",
    "    \"\"\"\n",
    "    def __init__(self, row, root_datapath):\n",
    "        self._data = row\n",
    "        self._path = os.path.join(root_datapath, row[0])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def path(self) -> str:\n",
    "        return self._path\n",
    "\n",
    "    @property\n",
    "    def num_frames(self) -> int:\n",
    "        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive\n",
    "    @property\n",
    "    def start_frame(self) -> int:\n",
    "        return int(self._data[1])\n",
    "\n",
    "    @property\n",
    "    def end_frame(self) -> int:\n",
    "        return int(self._data[2])\n",
    "\n",
    "    @property\n",
    "    def label(self) -> Union[int, List[int]]:\n",
    "        # just one label_id\n",
    "        if len(self._data) == 4:\n",
    "            return int(self._data[3])\n",
    "        # sample associated with multiple labels\n",
    "        else:\n",
    "            return [int(label_id) for label_id in self._data[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081c8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFrameDataset(torch.utils.data.Dataset):\n",
    "    r\"\"\"\n",
    "    A highly efficient and adaptable dataset class for videos.\n",
    "    Instead of loading every frame of a video,\n",
    "    loads x RGB frames of a video (sparse temporal sampling) and evenly\n",
    "    chooses those frames from start to end of the video, returning\n",
    "    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``\n",
    "    tensors where FRAMES=x if the ``ImglistToTensor()``\n",
    "    transform is used.\n",
    "\n",
    "    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS\n",
    "    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.\n",
    "\n",
    "    Note:\n",
    "        A demonstration of using this class can be seen\n",
    "        in ``demo.py``\n",
    "        https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch\n",
    "\n",
    "    Note:\n",
    "        This dataset broadly corresponds to the frame sampling technique\n",
    "        introduced in ``Temporal Segment Networks`` at ECCV2016\n",
    "        https://arxiv.org/abs/1608.00859.\n",
    "\n",
    "\n",
    "    Note:\n",
    "        This class relies on receiving video data in a structure where\n",
    "        inside a ``ROOT_DATA`` folder, each video lies in its own folder,\n",
    "        where each video folder contains the frames of the video as\n",
    "        individual files with a naming convention such as\n",
    "        img_001.jpg ... img_059.jpg.\n",
    "        For enumeration and annotations, this class expects to receive\n",
    "        the path to a .txt file where each video sample has a row with four\n",
    "        (or more in the case of multi-label, see README on Github)\n",
    "        space separated values:\n",
    "        ``VIDEO_FOLDER_PATH     START_FRAME      END_FRAME      LABEL_INDEX``.\n",
    "        ``VIDEO_FOLDER_PATH`` is expected to be the path of a video folder\n",
    "        excluding the ``ROOT_DATA`` prefix. For example, ``ROOT_DATA`` might\n",
    "        be ``home\\data\\datasetxyz\\videos\\``, inside of which a ``VIDEO_FOLDER_PATH``\n",
    "        might be ``jumping\\0052\\`` or ``sample1\\`` or ``00053\\``.\n",
    "\n",
    "    Args:\n",
    "        root_path: The root path in which video folders lie.\n",
    "                   this is ROOT_DATA from the description above.\n",
    "        annotationfile_path: The .txt annotation file containing\n",
    "                             one row per video sample as described above.\n",
    "        num_segments: The number of segments the video should\n",
    "                      be divided into to sample frames from.\n",
    "        frames_per_segment: The number of frames that should\n",
    "                            be loaded per segment. For each segment's\n",
    "                            frame-range, a random start index or the\n",
    "                            center is chosen, from which frames_per_segment\n",
    "                            consecutive frames are loaded.\n",
    "        imagefile_template: The image filename template that video frame files\n",
    "                            have inside of their video folders as described above.\n",
    "        transform: Transform pipeline that receives a list of PIL images/frames.\n",
    "        test_mode: If True, frames are taken from the center of each\n",
    "                   segment, instead of a random location in each segment.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root_path: str,\n",
    "                 annotationfile_path: str,\n",
    "                 num_segments: int = 3,\n",
    "                 frames_per_segment: int = 1,\n",
    "                 imagefile_template: str='img_{:05d}.jpg',\n",
    "                 transform = None,\n",
    "                 test_mode: bool = False):\n",
    "        super(VideoFrameDataset, self).__init__()\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.annotationfile_path = annotationfile_path\n",
    "        self.num_segments = num_segments\n",
    "        self.frames_per_segment = frames_per_segment\n",
    "        self.imagefile_template = imagefile_template\n",
    "        self.transform = transform\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "        self._parse_annotationfile()\n",
    "        self._sanity_check_samples()\n",
    "\n",
    "    def _load_image(self, directory: str, idx: int) -> Image.Image:\n",
    "        return Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert('RGB')\n",
    "\n",
    "    def _parse_annotationfile(self):\n",
    "        self.video_list = [VideoRecord(x.strip().split(), self.root_path) for x in open(self.annotationfile_path)]\n",
    "\n",
    "    def _sanity_check_samples(self):\n",
    "        for record in self.video_list:\n",
    "            if record.num_frames <= 0 or record.start_frame == record.end_frame:\n",
    "                print(f\"\\nDataset Warning: video {record.path} seems to have zero RGB frames on disk!\\n\")\n",
    "\n",
    "            elif record.num_frames < (self.num_segments * self.frames_per_segment):\n",
    "                print(f\"\\nDataset Warning: video {record.path} has {record.num_frames} frames \"\n",
    "                      f\"but the dataloader is set up to load \"\n",
    "                      f\"(num_segments={self.num_segments})*(frames_per_segment={self.frames_per_segment})\"\n",
    "                      f\"={self.num_segments * self.frames_per_segment} frames. Dataloader will throw an \"\n",
    "                      f\"error when trying to load this video.\\n\")\n",
    "\n",
    "    def _get_start_indices(self, record: VideoRecord) -> 'np.ndarray[int]':\n",
    "        \"\"\"\n",
    "        For each segment, choose a start index from where frames\n",
    "        are to be loaded from.\n",
    "\n",
    "        Args:\n",
    "            record: VideoRecord denoting a video sample.\n",
    "        Returns:\n",
    "            List of indices of where the frames of each\n",
    "            segment are to be loaded from.\n",
    "        \"\"\"\n",
    "        # choose start indices that are perfectly evenly spread across the video frames.\n",
    "        if self.test_mode:\n",
    "            distance_between_indices = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)\n",
    "\n",
    "            start_indices = np.array([int(distance_between_indices / 2.0 + distance_between_indices * x)\n",
    "                                      for x in range(self.num_segments)])\n",
    "        # randomly sample start indices that are approximately evenly spread across the video frames.\n",
    "        else:\n",
    "            max_valid_start_index = (record.num_frames - self.frames_per_segment + 1) // self.num_segments\n",
    "\n",
    "            start_indices = np.multiply(list(range(self.num_segments)), max_valid_start_index) + \\\n",
    "                      np.random.randint(max_valid_start_index, size=self.num_segments)\n",
    "\n",
    "        return start_indices\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Union[\n",
    "        Tuple[List[Image.Image], Union[int, List[int]]],\n",
    "        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n",
    "        Tuple[Any, Union[int, List[int]]],\n",
    "        ]:\n",
    "        \"\"\"\n",
    "        For video with id idx, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT\n",
    "        frames from evenly chosen locations across the video.\n",
    "\n",
    "        Args:\n",
    "            idx: Video sample index.\n",
    "        Returns:\n",
    "            A tuple of (video, label). Label is either a single\n",
    "            integer or a list of integers in the case of multiple labels.\n",
    "            Video is either 1) a list of PIL images if no transform is used\n",
    "            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n",
    "            if the transform \"ImglistToTensor\" is used\n",
    "            3) or anything else if a custom transform is used.\n",
    "        \"\"\"\n",
    "        record: VideoRecord = self.video_list[idx]\n",
    "\n",
    "        frame_start_indices: 'np.ndarray[int]' = self._get_start_indices(record)\n",
    "\n",
    "        return self._get(record, frame_start_indices)\n",
    "\n",
    "    def _get(self, record: VideoRecord, frame_start_indices: 'np.ndarray[int]') -> Union[\n",
    "        Tuple[List[Image.Image], Union[int, List[int]]],\n",
    "        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n",
    "        Tuple[Any, Union[int, List[int]]],\n",
    "        ]:\n",
    "        \"\"\"\n",
    "        Loads the frames of a video at the corresponding\n",
    "        indices.\n",
    "\n",
    "        Args:\n",
    "            record: VideoRecord denoting a video sample.\n",
    "            frame_start_indices: Indices from which to load consecutive frames from.\n",
    "        Returns:\n",
    "            A tuple of (video, label). Label is either a single\n",
    "            integer or a list of integers in the case of multiple labels.\n",
    "            Video is either 1) a list of PIL images if no transform is used\n",
    "            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n",
    "            if the transform \"ImglistToTensor\" is used\n",
    "            3) or anything else if a custom transform is used.\n",
    "        \"\"\"\n",
    "\n",
    "        frame_start_indices = frame_start_indices + record.start_frame\n",
    "        images = list()\n",
    "\n",
    "        # from each start_index, load self.frames_per_segment\n",
    "        # consecutive frames\n",
    "        for start_index in frame_start_indices:\n",
    "            frame_index = int(start_index)\n",
    "\n",
    "            # load self.frames_per_segment consecutive frames\n",
    "            for _ in range(self.frames_per_segment):\n",
    "                image = self._load_image(record.path, frame_index)\n",
    "                images.append(image)\n",
    "\n",
    "                if frame_index < record.end_frame:\n",
    "                    frame_index += 1\n",
    "\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(images)\n",
    "\n",
    "        return images, record.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01236e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "def eval_preprocess(size):\n",
    "    preprocess = transforms.Compose([\n",
    "        ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        transforms.Resize(size),  # image batch, resize smaller edge to 299\n",
    "        transforms.CenterCrop(size),  # image batch, center crop to square 299x299\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ShiftWithChannelTensor()\n",
    "    ])\n",
    "    return preprocess\n",
    "\n",
    "def train_preprocess(size):\n",
    "    train_preprocess = transforms.Compose([\n",
    "        ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        transforms.Resize(size),  # image batch, resize smaller edge to 299\n",
    "        transforms.RandomCrop(size),  # image batch, center crop to square 299x299\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ShiftWithChannelTensor()\n",
    "    ])\n",
    "    return train_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03c70608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "PATH = 'model_state_dict.pt'\n",
    "device = torch.device('cuda')\n",
    "model = MobileNetV2(num_classes=6, sample_size=112, width_mult=1.0)\n",
    "model.load_state_dict(torch.load(PATH),strict=False)\n",
    "model.to(device)\n",
    "# freezing layer\n",
    "ct = 0\n",
    "for child in model.children():\n",
    "    for param in child.parameters():\n",
    "            ct +=1\n",
    "            if ct <120:\n",
    "                param.requires_grad = False\n",
    "optimizer = optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38fb626f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport glob\\nfrom os.path import normpath, basename\\nfrom sklearn.model_selection import train_test_split\\n\\nDATA_PATH = \\'transfer/activities\\'#dataset\\n# create dictionary of activities\\n\\nlist_activities= os.listdir(DATA_PATH)\\nlist_dict= {}\\nfor index,activity in enumerate(list_activities):\\n    list_dict[activity] = index\\n    \\n# create a note \\nwith open(\\'label_notes.txt\\', \\'w\\') as f:\\n    for key,value in list_dict.items():\\n        f.write(f\"{key}: {value}\")\\n        f.write(\\'\\n\\')\\n\\n# create list of data\\nall_x =[]\\nall_y = []\\nfor path, subdirs, files in os.walk(DATA_PATH):\\n    for name in files:\\n        all_x.append(os.path.join(path, name))\\n        all_y.append(list_dict[basename(normpath(path))])\\n\\nprint(f\"Currently have {len(all_x)} video data...\")  \\n\\n# split to train and test\\nX_train, X_test, y_train, y_test = train_test_split(all_x, all_y, test_size=0.2, random_state=42, stratify=all_y)\\n\\n# generate image for train & test\\ncheck_mkdir(\\'transfer/train\\')#dataset/train\\ncheck_mkdir(\\'transfer/test\\')#dataset/test\\n\\nfor key,value in list_dict.items():\\n    check_mkdir(os.path.join(\\'transfer/train\\',str(value)))\\n    check_mkdir(os.path.join(\\'transfer/test\\',str(value)))\\n\\nwith open(\\'transfer/train/annotations.txt\\', \\'w\\') as f:\\n    for index,video in enumerate(X_train):\\n        vid_in_folder = len(os.listdir(os.path.join(\\'transfer/train\\',str(y_train[index]))))\\n        path_folder = os.path.join(\\'transfer/train\\',str(y_train[index]),str(vid_in_folder+1).zfill(5))\\n        check_mkdir(path_folder)\\n        # parse video into frame\\n        last_frame = create_frames_from_video(video,path_folder)\\n        # create note\\n        f.write(f\\'{y_train[index]}/{str(vid_in_folder+1).zfill(5)} 1 {last_frame-1} {y_train[index]}\\')\\n        f.write(\\'\\n\\')\\n        \\nwith open(\\'transfer/test/annotations.txt\\', \\'w\\') as f:\\n    for index,video in enumerate(X_test):\\n        vid_in_folder = len(os.listdir(os.path.join(\\'transfer/test\\',str(y_test[index]))))\\n        path_folder = os.path.join(\\'transfer/test\\',str(y_test[index]),str(vid_in_folder+1).zfill(5))\\n        check_mkdir(path_folder)\\n        # parse video into frame\\n        last_frame = create_frames_from_video(video,path_folder)\\n        # create note\\n        f.write(f\\'{y_test[index]}/{str(vid_in_folder+1).zfill(5)} 1 {last_frame-1} {y_test[index]}\\')\\n        f.write(\\'\\n\\')'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import glob\n",
    "from os.path import normpath, basename\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_PATH = 'transfer/activities'#dataset\n",
    "# create dictionary of activities\n",
    "\n",
    "list_activities= os.listdir(DATA_PATH)\n",
    "list_dict= {}\n",
    "for index,activity in enumerate(list_activities):\n",
    "    list_dict[activity] = index\n",
    "    \n",
    "# create a note \n",
    "with open('label_notes.txt', 'w') as f:\n",
    "    for key,value in list_dict.items():\n",
    "        f.write(f\"{key}: {value}\")\n",
    "        f.write('\\n')\n",
    "\n",
    "# create list of data\n",
    "all_x =[]\n",
    "all_y = []\n",
    "for path, subdirs, files in os.walk(DATA_PATH):\n",
    "    for name in files:\n",
    "        all_x.append(os.path.join(path, name))\n",
    "        all_y.append(list_dict[basename(normpath(path))])\n",
    "\n",
    "print(f\"Currently have {len(all_x)} video data...\")  \n",
    "\n",
    "# split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_x, all_y, test_size=0.2, random_state=42, stratify=all_y)\n",
    "\n",
    "# generate image for train & test\n",
    "check_mkdir('transfer/train')#dataset/train\n",
    "check_mkdir('transfer/test')#dataset/test\n",
    "\n",
    "for key,value in list_dict.items():\n",
    "    check_mkdir(os.path.join('transfer/train',str(value)))\n",
    "    check_mkdir(os.path.join('transfer/test',str(value)))\n",
    "\n",
    "with open('transfer/train/annotations.txt', 'w') as f:\n",
    "    for index,video in enumerate(X_train):\n",
    "        vid_in_folder = len(os.listdir(os.path.join('transfer/train',str(y_train[index]))))\n",
    "        path_folder = os.path.join('transfer/train',str(y_train[index]),str(vid_in_folder+1).zfill(5))\n",
    "        check_mkdir(path_folder)\n",
    "        # parse video into frame\n",
    "        last_frame = create_frames_from_video(video,path_folder)\n",
    "        # create note\n",
    "        f.write(f'{y_train[index]}/{str(vid_in_folder+1).zfill(5)} 1 {last_frame-1} {y_train[index]}')\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open('transfer/test/annotations.txt', 'w') as f:\n",
    "    for index,video in enumerate(X_test):\n",
    "        vid_in_folder = len(os.listdir(os.path.join('transfer/test',str(y_test[index]))))\n",
    "        path_folder = os.path.join('transfer/test',str(y_test[index]),str(vid_in_folder+1).zfill(5))\n",
    "        check_mkdir(path_folder)\n",
    "        # parse video into frame\n",
    "        last_frame = create_frames_from_video(video,path_folder)\n",
    "        # create note\n",
    "        f.write(f'{y_test[index]}/{str(vid_in_folder+1).zfill(5)} 1 {last_frame-1} {y_test[index]}')\n",
    "        f.write('\\n')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf0ff9d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on gpu\n",
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch17_p38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.396008 \tTraining acc: 0.45 \tValidation Loss: 1.502460 \tAccuracy: 0.43 \tF1-Score: 0.17\n",
      "Validation loss decreased (inf --> 1.502460).  Saving model ...\n",
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch17_p38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 1.268910 \tTraining acc: 0.48 \tValidation Loss: 1.554731 \tAccuracy: 0.42 \tF1-Score: 0.25\n",
      "validation\n",
      "Epoch: 3 \tTraining Loss: 1.170239 \tTraining acc: 0.52 \tValidation Loss: 1.417397 \tAccuracy: 0.45 \tF1-Score: 0.39\n",
      "Validation loss decreased (1.502460 --> 1.417397).  Saving model ...\n",
      "validation\n",
      "Epoch: 4 \tTraining Loss: 1.079528 \tTraining acc: 0.55 \tValidation Loss: 1.395337 \tAccuracy: 0.50 \tF1-Score: 0.49\n",
      "Validation loss decreased (1.417397 --> 1.395337).  Saving model ...\n",
      "validation\n",
      "Epoch: 5 \tTraining Loss: 1.002751 \tTraining acc: 0.59 \tValidation Loss: 1.180161 \tAccuracy: 0.54 \tF1-Score: 0.58\n",
      "Validation loss decreased (1.395337 --> 1.180161).  Saving model ...\n",
      "validation\n",
      "Epoch: 6 \tTraining Loss: 0.936023 \tTraining acc: 0.62 \tValidation Loss: 1.371934 \tAccuracy: 0.50 \tF1-Score: 0.51\n",
      "validation\n",
      "Epoch: 7 \tTraining Loss: 0.877579 \tTraining acc: 0.64 \tValidation Loss: 1.138179 \tAccuracy: 0.56 \tF1-Score: 0.57\n",
      "Validation loss decreased (1.180161 --> 1.138179).  Saving model ...\n",
      "validation\n",
      "Epoch: 8 \tTraining Loss: 0.856800 \tTraining acc: 0.66 \tValidation Loss: 1.007183 \tAccuracy: 0.61 \tF1-Score: 0.63\n",
      "Validation loss decreased (1.138179 --> 1.007183).  Saving model ...\n",
      "validation\n",
      "Epoch: 9 \tTraining Loss: 0.813398 \tTraining acc: 0.68 \tValidation Loss: 1.054917 \tAccuracy: 0.57 \tF1-Score: 0.60\n",
      "validation\n",
      "Epoch: 10 \tTraining Loss: 0.746322 \tTraining acc: 0.70 \tValidation Loss: 1.039517 \tAccuracy: 0.62 \tF1-Score: 0.65\n",
      "validation\n",
      "Epoch: 11 \tTraining Loss: 0.735797 \tTraining acc: 0.70 \tValidation Loss: 1.062727 \tAccuracy: 0.58 \tF1-Score: 0.63\n",
      "validation\n",
      "Epoch: 12 \tTraining Loss: 0.726316 \tTraining acc: 0.71 \tValidation Loss: 0.921795 \tAccuracy: 0.63 \tF1-Score: 0.62\n",
      "Validation loss decreased (1.007183 --> 0.921795).  Saving model ...\n",
      "validation\n",
      "Epoch: 13 \tTraining Loss: 0.692887 \tTraining acc: 0.72 \tValidation Loss: 0.949758 \tAccuracy: 0.64 \tF1-Score: 0.66\n",
      "validation\n",
      "Epoch: 14 \tTraining Loss: 0.667815 \tTraining acc: 0.73 \tValidation Loss: 1.057457 \tAccuracy: 0.64 \tF1-Score: 0.66\n",
      "validation\n",
      "Epoch: 15 \tTraining Loss: 0.652513 \tTraining acc: 0.74 \tValidation Loss: 1.040773 \tAccuracy: 0.64 \tF1-Score: 0.64\n",
      "validation\n",
      "Epoch: 16 \tTraining Loss: 0.640040 \tTraining acc: 0.75 \tValidation Loss: 0.942744 \tAccuracy: 0.65 \tF1-Score: 0.61\n",
      "validation\n",
      "Epoch: 17 \tTraining Loss: 0.625617 \tTraining acc: 0.75 \tValidation Loss: 0.928877 \tAccuracy: 0.64 \tF1-Score: 0.60\n",
      "validation\n",
      "Epoch: 18 \tTraining Loss: 0.615018 \tTraining acc: 0.75 \tValidation Loss: 0.903751 \tAccuracy: 0.68 \tF1-Score: 0.69\n",
      "Validation loss decreased (0.921795 --> 0.903751).  Saving model ...\n",
      "validation\n",
      "Epoch: 19 \tTraining Loss: 0.584794 \tTraining acc: 0.77 \tValidation Loss: 0.930611 \tAccuracy: 0.66 \tF1-Score: 0.68\n",
      "validation\n",
      "Epoch: 20 \tTraining Loss: 0.592811 \tTraining acc: 0.76 \tValidation Loss: 0.930591 \tAccuracy: 0.68 \tF1-Score: 0.67\n",
      "validation\n",
      "Epoch: 21 \tTraining Loss: 0.579511 \tTraining acc: 0.77 \tValidation Loss: 0.846549 \tAccuracy: 0.68 \tF1-Score: 0.70\n",
      "Validation loss decreased (0.903751 --> 0.846549).  Saving model ...\n",
      "validation\n",
      "Epoch: 22 \tTraining Loss: 0.548490 \tTraining acc: 0.78 \tValidation Loss: 0.927479 \tAccuracy: 0.66 \tF1-Score: 0.65\n",
      "validation\n",
      "Epoch: 23 \tTraining Loss: 0.541968 \tTraining acc: 0.78 \tValidation Loss: 0.839342 \tAccuracy: 0.69 \tF1-Score: 0.68\n",
      "Validation loss decreased (0.846549 --> 0.839342).  Saving model ...\n",
      "validation\n",
      "Epoch: 24 \tTraining Loss: 0.535512 \tTraining acc: 0.78 \tValidation Loss: 0.934322 \tAccuracy: 0.68 \tF1-Score: 0.67\n",
      "validation\n",
      "Epoch: 25 \tTraining Loss: 0.544409 \tTraining acc: 0.78 \tValidation Loss: 0.779901 \tAccuracy: 0.72 \tF1-Score: 0.74\n",
      "Validation loss decreased (0.839342 --> 0.779901).  Saving model ...\n",
      "validation\n",
      "Epoch: 26 \tTraining Loss: 0.516469 \tTraining acc: 0.79 \tValidation Loss: 0.919918 \tAccuracy: 0.67 \tF1-Score: 0.67\n",
      "validation\n",
      "Epoch: 27 \tTraining Loss: 0.504842 \tTraining acc: 0.80 \tValidation Loss: 0.952497 \tAccuracy: 0.69 \tF1-Score: 0.68\n",
      "validation\n",
      "Epoch: 28 \tTraining Loss: 0.505038 \tTraining acc: 0.79 \tValidation Loss: 0.829227 \tAccuracy: 0.72 \tF1-Score: 0.73\n",
      "validation\n",
      "Epoch: 29 \tTraining Loss: 0.495145 \tTraining acc: 0.80 \tValidation Loss: 0.907690 \tAccuracy: 0.70 \tF1-Score: 0.68\n",
      "validation\n",
      "Epoch: 30 \tTraining Loss: 0.497318 \tTraining acc: 0.80 \tValidation Loss: 0.821343 \tAccuracy: 0.71 \tF1-Score: 0.72\n",
      "validation\n",
      "Epoch: 31 \tTraining Loss: 0.491487 \tTraining acc: 0.80 \tValidation Loss: 0.876478 \tAccuracy: 0.69 \tF1-Score: 0.70\n",
      "validation\n",
      "Epoch: 32 \tTraining Loss: 0.477674 \tTraining acc: 0.80 \tValidation Loss: 0.932936 \tAccuracy: 0.69 \tF1-Score: 0.70\n",
      "validation\n",
      "Epoch: 33 \tTraining Loss: 0.460149 \tTraining acc: 0.81 \tValidation Loss: 1.004995 \tAccuracy: 0.67 \tF1-Score: 0.68\n",
      "validation\n",
      "Epoch: 34 \tTraining Loss: 0.466200 \tTraining acc: 0.81 \tValidation Loss: 1.118193 \tAccuracy: 0.67 \tF1-Score: 0.64\n",
      "validation\n",
      "Epoch: 35 \tTraining Loss: 0.456304 \tTraining acc: 0.82 \tValidation Loss: 0.896727 \tAccuracy: 0.67 \tF1-Score: 0.67\n",
      "validation\n",
      "Epoch: 36 \tTraining Loss: 0.455565 \tTraining acc: 0.81 \tValidation Loss: 0.726218 \tAccuracy: 0.72 \tF1-Score: 0.76\n",
      "Validation loss decreased (0.779901 --> 0.726218).  Saving model ...\n",
      "validation\n",
      "Epoch: 37 \tTraining Loss: 0.447325 \tTraining acc: 0.82 \tValidation Loss: 0.909848 \tAccuracy: 0.72 \tF1-Score: 0.72\n",
      "validation\n",
      "Epoch: 38 \tTraining Loss: 0.444770 \tTraining acc: 0.82 \tValidation Loss: 0.928424 \tAccuracy: 0.70 \tF1-Score: 0.69\n",
      "validation\n",
      "Epoch: 39 \tTraining Loss: 0.437133 \tTraining acc: 0.82 \tValidation Loss: 0.934701 \tAccuracy: 0.70 \tF1-Score: 0.72\n",
      "validation\n",
      "Epoch: 40 \tTraining Loss: 0.417530 \tTraining acc: 0.83 \tValidation Loss: 1.025584 \tAccuracy: 0.68 \tF1-Score: 0.68\n",
      "validation\n",
      "Epoch: 41 \tTraining Loss: 0.423573 \tTraining acc: 0.83 \tValidation Loss: 0.772072 \tAccuracy: 0.73 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 42 \tTraining Loss: 0.405105 \tTraining acc: 0.83 \tValidation Loss: 0.724967 \tAccuracy: 0.72 \tF1-Score: 0.75\n",
      "Validation loss decreased (0.726218 --> 0.724967).  Saving model ...\n",
      "validation\n",
      "Epoch: 43 \tTraining Loss: 0.418606 \tTraining acc: 0.84 \tValidation Loss: 0.814280 \tAccuracy: 0.70 \tF1-Score: 0.71\n",
      "validation\n",
      "Epoch: 44 \tTraining Loss: 0.428428 \tTraining acc: 0.83 \tValidation Loss: 0.822365 \tAccuracy: 0.72 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 45 \tTraining Loss: 0.416721 \tTraining acc: 0.83 \tValidation Loss: 0.969884 \tAccuracy: 0.70 \tF1-Score: 0.72\n",
      "validation\n",
      "Epoch: 46 \tTraining Loss: 0.407182 \tTraining acc: 0.84 \tValidation Loss: 0.862094 \tAccuracy: 0.70 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 47 \tTraining Loss: 0.387444 \tTraining acc: 0.84 \tValidation Loss: 0.797616 \tAccuracy: 0.72 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 48 \tTraining Loss: 0.373070 \tTraining acc: 0.85 \tValidation Loss: 0.769715 \tAccuracy: 0.72 \tF1-Score: 0.71\n",
      "validation\n",
      "Epoch: 49 \tTraining Loss: 0.383674 \tTraining acc: 0.84 \tValidation Loss: 0.798783 \tAccuracy: 0.72 \tF1-Score: 0.70\n",
      "validation\n",
      "Epoch: 50 \tTraining Loss: 0.385971 \tTraining acc: 0.84 \tValidation Loss: 0.849022 \tAccuracy: 0.73 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 51 \tTraining Loss: 0.380065 \tTraining acc: 0.84 \tValidation Loss: 0.860315 \tAccuracy: 0.69 \tF1-Score: 0.70\n",
      "validation\n",
      "Epoch: 52 \tTraining Loss: 0.367213 \tTraining acc: 0.85 \tValidation Loss: 0.797096 \tAccuracy: 0.73 \tF1-Score: 0.73\n",
      "validation\n",
      "Epoch: 53 \tTraining Loss: 0.379039 \tTraining acc: 0.85 \tValidation Loss: 0.866305 \tAccuracy: 0.69 \tF1-Score: 0.70\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-05.\n",
      "validation\n",
      "Epoch: 54 \tTraining Loss: 0.348068 \tTraining acc: 0.86 \tValidation Loss: 0.799592 \tAccuracy: 0.73 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 55 \tTraining Loss: 0.322750 \tTraining acc: 0.86 \tValidation Loss: 0.756635 \tAccuracy: 0.73 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 56 \tTraining Loss: 0.310843 \tTraining acc: 0.87 \tValidation Loss: 0.858047 \tAccuracy: 0.73 \tF1-Score: 0.73\n",
      "validation\n",
      "Epoch: 57 \tTraining Loss: 0.308330 \tTraining acc: 0.87 \tValidation Loss: 0.806815 \tAccuracy: 0.72 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 58 \tTraining Loss: 0.304357 \tTraining acc: 0.88 \tValidation Loss: 0.728541 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 59 \tTraining Loss: 0.289966 \tTraining acc: 0.88 \tValidation Loss: 0.753463 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 60 \tTraining Loss: 0.296311 \tTraining acc: 0.88 \tValidation Loss: 0.811102 \tAccuracy: 0.74 \tF1-Score: 0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "Epoch: 61 \tTraining Loss: 0.286395 \tTraining acc: 0.88 \tValidation Loss: 0.776412 \tAccuracy: 0.73 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 62 \tTraining Loss: 0.301796 \tTraining acc: 0.88 \tValidation Loss: 0.751565 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 63 \tTraining Loss: 0.283289 \tTraining acc: 0.88 \tValidation Loss: 0.721796 \tAccuracy: 0.76 \tF1-Score: 0.78\n",
      "Validation loss decreased (0.724967 --> 0.721796).  Saving model ...\n",
      "validation\n",
      "Epoch: 64 \tTraining Loss: 0.284210 \tTraining acc: 0.88 \tValidation Loss: 0.820832 \tAccuracy: 0.72 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 65 \tTraining Loss: 0.297467 \tTraining acc: 0.88 \tValidation Loss: 0.840889 \tAccuracy: 0.70 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 66 \tTraining Loss: 0.283615 \tTraining acc: 0.89 \tValidation Loss: 0.821976 \tAccuracy: 0.74 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 67 \tTraining Loss: 0.292987 \tTraining acc: 0.89 \tValidation Loss: 0.750855 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 68 \tTraining Loss: 0.299370 \tTraining acc: 0.87 \tValidation Loss: 0.832535 \tAccuracy: 0.73 \tF1-Score: 0.73\n",
      "validation\n",
      "Epoch: 69 \tTraining Loss: 0.281587 \tTraining acc: 0.88 \tValidation Loss: 0.758736 \tAccuracy: 0.73 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 70 \tTraining Loss: 0.276535 \tTraining acc: 0.89 \tValidation Loss: 0.798370 \tAccuracy: 0.73 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 71 \tTraining Loss: 0.285287 \tTraining acc: 0.89 \tValidation Loss: 0.772053 \tAccuracy: 0.73 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 72 \tTraining Loss: 0.279859 \tTraining acc: 0.88 \tValidation Loss: 0.804809 \tAccuracy: 0.71 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 73 \tTraining Loss: 0.281467 \tTraining acc: 0.89 \tValidation Loss: 0.825571 \tAccuracy: 0.72 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 74 \tTraining Loss: 0.280859 \tTraining acc: 0.88 \tValidation Loss: 0.772983 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "validation\n",
      "Epoch: 75 \tTraining Loss: 0.281730 \tTraining acc: 0.88 \tValidation Loss: 0.772695 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 76 \tTraining Loss: 0.275616 \tTraining acc: 0.88 \tValidation Loss: 0.775637 \tAccuracy: 0.73 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 77 \tTraining Loss: 0.273320 \tTraining acc: 0.89 \tValidation Loss: 0.812530 \tAccuracy: 0.73 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 78 \tTraining Loss: 0.271957 \tTraining acc: 0.89 \tValidation Loss: 0.800812 \tAccuracy: 0.72 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 79 \tTraining Loss: 0.271141 \tTraining acc: 0.89 \tValidation Loss: 0.792993 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 80 \tTraining Loss: 0.264967 \tTraining acc: 0.89 \tValidation Loss: 0.789335 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 81 \tTraining Loss: 0.279127 \tTraining acc: 0.88 \tValidation Loss: 0.774009 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 82 \tTraining Loss: 0.264965 \tTraining acc: 0.89 \tValidation Loss: 0.823026 \tAccuracy: 0.75 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 83 \tTraining Loss: 0.261933 \tTraining acc: 0.89 \tValidation Loss: 0.721360 \tAccuracy: 0.75 \tF1-Score: 0.77\n",
      "Validation loss decreased (0.721796 --> 0.721360).  Saving model ...\n",
      "validation\n",
      "Epoch: 84 \tTraining Loss: 0.274934 \tTraining acc: 0.89 \tValidation Loss: 0.712062 \tAccuracy: 0.76 \tF1-Score: 0.79\n",
      "Validation loss decreased (0.721360 --> 0.712062).  Saving model ...\n",
      "validation\n",
      "Epoch: 85 \tTraining Loss: 0.270199 \tTraining acc: 0.89 \tValidation Loss: 0.867882 \tAccuracy: 0.72 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 86 \tTraining Loss: 0.266129 \tTraining acc: 0.89 \tValidation Loss: 0.785037 \tAccuracy: 0.73 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 87 \tTraining Loss: 0.270536 \tTraining acc: 0.89 \tValidation Loss: 0.798566 \tAccuracy: 0.73 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 88 \tTraining Loss: 0.261947 \tTraining acc: 0.89 \tValidation Loss: 0.789867 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 89 \tTraining Loss: 0.268323 \tTraining acc: 0.89 \tValidation Loss: 0.763242 \tAccuracy: 0.72 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 90 \tTraining Loss: 0.268431 \tTraining acc: 0.89 \tValidation Loss: 0.736049 \tAccuracy: 0.75 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 91 \tTraining Loss: 0.271119 \tTraining acc: 0.89 \tValidation Loss: 0.772707 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 92 \tTraining Loss: 0.266683 \tTraining acc: 0.89 \tValidation Loss: 0.795149 \tAccuracy: 0.73 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 93 \tTraining Loss: 0.266048 \tTraining acc: 0.89 \tValidation Loss: 0.744455 \tAccuracy: 0.75 \tF1-Score: 0.78\n",
      "validation\n",
      "Epoch: 94 \tTraining Loss: 0.265918 \tTraining acc: 0.90 \tValidation Loss: 0.755875 \tAccuracy: 0.73 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 95 \tTraining Loss: 0.264166 \tTraining acc: 0.90 \tValidation Loss: 0.835956 \tAccuracy: 0.72 \tF1-Score: 0.76\n",
      "Epoch 00095: reducing learning rate of group 0 to 1.0000e-07.\n",
      "validation\n",
      "Epoch: 96 \tTraining Loss: 0.264074 \tTraining acc: 0.90 \tValidation Loss: 0.798200 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 97 \tTraining Loss: 0.277000 \tTraining acc: 0.88 \tValidation Loss: 0.737002 \tAccuracy: 0.75 \tF1-Score: 0.78\n",
      "validation\n",
      "Epoch: 98 \tTraining Loss: 0.262681 \tTraining acc: 0.89 \tValidation Loss: 0.720990 \tAccuracy: 0.76 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 99 \tTraining Loss: 0.265029 \tTraining acc: 0.89 \tValidation Loss: 0.845128 \tAccuracy: 0.73 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 100 \tTraining Loss: 0.247088 \tTraining acc: 0.90 \tValidation Loss: 0.761901 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 101 \tTraining Loss: 0.255176 \tTraining acc: 0.89 \tValidation Loss: 0.733707 \tAccuracy: 0.73 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 102 \tTraining Loss: 0.256171 \tTraining acc: 0.89 \tValidation Loss: 0.703267 \tAccuracy: 0.76 \tF1-Score: 0.79\n",
      "Validation loss decreased (0.712062 --> 0.703267).  Saving model ...\n",
      "validation\n",
      "Epoch: 103 \tTraining Loss: 0.268303 \tTraining acc: 0.89 \tValidation Loss: 0.756879 \tAccuracy: 0.74 \tF1-Score: 0.78\n",
      "validation\n",
      "Epoch: 104 \tTraining Loss: 0.271715 \tTraining acc: 0.89 \tValidation Loss: 0.804154 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 105 \tTraining Loss: 0.265613 \tTraining acc: 0.89 \tValidation Loss: 0.777226 \tAccuracy: 0.75 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 106 \tTraining Loss: 0.264077 \tTraining acc: 0.89 \tValidation Loss: 0.826649 \tAccuracy: 0.74 \tF1-Score: 0.78\n",
      "validation\n",
      "Epoch: 107 \tTraining Loss: 0.266248 \tTraining acc: 0.89 \tValidation Loss: 0.790767 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 108 \tTraining Loss: 0.269405 \tTraining acc: 0.89 \tValidation Loss: 0.751219 \tAccuracy: 0.76 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 109 \tTraining Loss: 0.262485 \tTraining acc: 0.89 \tValidation Loss: 0.757202 \tAccuracy: 0.72 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 110 \tTraining Loss: 0.265736 \tTraining acc: 0.89 \tValidation Loss: 0.757550 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 111 \tTraining Loss: 0.275445 \tTraining acc: 0.89 \tValidation Loss: 0.787492 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 112 \tTraining Loss: 0.258619 \tTraining acc: 0.89 \tValidation Loss: 0.738540 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 113 \tTraining Loss: 0.258453 \tTraining acc: 0.89 \tValidation Loss: 0.716898 \tAccuracy: 0.75 \tF1-Score: 0.78\n",
      "Epoch 00113: reducing learning rate of group 0 to 1.0000e-08.\n",
      "validation\n",
      "Epoch: 114 \tTraining Loss: 0.278907 \tTraining acc: 0.89 \tValidation Loss: 0.783684 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 115 \tTraining Loss: 0.261044 \tTraining acc: 0.89 \tValidation Loss: 0.758529 \tAccuracy: 0.75 \tF1-Score: 0.79\n",
      "validation\n",
      "Epoch: 116 \tTraining Loss: 0.269114 \tTraining acc: 0.89 \tValidation Loss: 0.722333 \tAccuracy: 0.76 \tF1-Score: 0.78\n",
      "validation\n",
      "Epoch: 117 \tTraining Loss: 0.267365 \tTraining acc: 0.89 \tValidation Loss: 0.804456 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 118 \tTraining Loss: 0.265395 \tTraining acc: 0.89 \tValidation Loss: 0.763341 \tAccuracy: 0.74 \tF1-Score: 0.79\n",
      "validation\n",
      "Epoch: 119 \tTraining Loss: 0.254257 \tTraining acc: 0.90 \tValidation Loss: 0.815477 \tAccuracy: 0.72 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 120 \tTraining Loss: 0.277002 \tTraining acc: 0.89 \tValidation Loss: 0.801171 \tAccuracy: 0.73 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 121 \tTraining Loss: 0.259846 \tTraining acc: 0.89 \tValidation Loss: 0.778689 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 122 \tTraining Loss: 0.265047 \tTraining acc: 0.89 \tValidation Loss: 0.880820 \tAccuracy: 0.73 \tF1-Score: 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "Epoch: 123 \tTraining Loss: 0.262791 \tTraining acc: 0.89 \tValidation Loss: 0.804562 \tAccuracy: 0.72 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 124 \tTraining Loss: 0.260474 \tTraining acc: 0.89 \tValidation Loss: 0.742497 \tAccuracy: 0.74 \tF1-Score: 0.79\n",
      "validation\n",
      "Epoch: 125 \tTraining Loss: 0.265810 \tTraining acc: 0.89 \tValidation Loss: 0.826153 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 126 \tTraining Loss: 0.265099 \tTraining acc: 0.89 \tValidation Loss: 0.858387 \tAccuracy: 0.72 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 127 \tTraining Loss: 0.257599 \tTraining acc: 0.90 \tValidation Loss: 0.854647 \tAccuracy: 0.72 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 128 \tTraining Loss: 0.262190 \tTraining acc: 0.89 \tValidation Loss: 0.741840 \tAccuracy: 0.75 \tF1-Score: 0.78\n",
      "validation\n",
      "Epoch: 129 \tTraining Loss: 0.265143 \tTraining acc: 0.89 \tValidation Loss: 0.822776 \tAccuracy: 0.75 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 130 \tTraining Loss: 0.262679 \tTraining acc: 0.90 \tValidation Loss: 0.749655 \tAccuracy: 0.74 \tF1-Score: 0.78\n",
      "validation\n",
      "Epoch: 131 \tTraining Loss: 0.272041 \tTraining acc: 0.89 \tValidation Loss: 0.764801 \tAccuracy: 0.73 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 132 \tTraining Loss: 0.271139 \tTraining acc: 0.89 \tValidation Loss: 0.826043 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 133 \tTraining Loss: 0.268631 \tTraining acc: 0.89 \tValidation Loss: 0.747083 \tAccuracy: 0.74 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 134 \tTraining Loss: 0.261569 \tTraining acc: 0.89 \tValidation Loss: 0.757887 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 135 \tTraining Loss: 0.250637 \tTraining acc: 0.89 \tValidation Loss: 0.913201 \tAccuracy: 0.72 \tF1-Score: 0.73\n",
      "validation\n",
      "Epoch: 136 \tTraining Loss: 0.264665 \tTraining acc: 0.90 \tValidation Loss: 0.859347 \tAccuracy: 0.73 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 137 \tTraining Loss: 0.263301 \tTraining acc: 0.89 \tValidation Loss: 0.781981 \tAccuracy: 0.73 \tF1-Score: 0.74\n",
      "validation\n",
      "Epoch: 138 \tTraining Loss: 0.262981 \tTraining acc: 0.89 \tValidation Loss: 0.753778 \tAccuracy: 0.75 \tF1-Score: 0.78\n",
      "validation\n",
      "Epoch: 139 \tTraining Loss: 0.269592 \tTraining acc: 0.89 \tValidation Loss: 0.782313 \tAccuracy: 0.75 \tF1-Score: 0.79\n",
      "validation\n",
      "Epoch: 140 \tTraining Loss: 0.255985 \tTraining acc: 0.89 \tValidation Loss: 0.764619 \tAccuracy: 0.75 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 141 \tTraining Loss: 0.259745 \tTraining acc: 0.89 \tValidation Loss: 0.749565 \tAccuracy: 0.74 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 142 \tTraining Loss: 0.272408 \tTraining acc: 0.89 \tValidation Loss: 0.811049 \tAccuracy: 0.73 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 143 \tTraining Loss: 0.272753 \tTraining acc: 0.89 \tValidation Loss: 0.785018 \tAccuracy: 0.75 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 144 \tTraining Loss: 0.260959 \tTraining acc: 0.90 \tValidation Loss: 0.805493 \tAccuracy: 0.73 \tF1-Score: 0.75\n",
      "validation\n",
      "Epoch: 145 \tTraining Loss: 0.266993 \tTraining acc: 0.89 \tValidation Loss: 0.785853 \tAccuracy: 0.73 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 146 \tTraining Loss: 0.257558 \tTraining acc: 0.90 \tValidation Loss: 0.786460 \tAccuracy: 0.75 \tF1-Score: 0.79\n",
      "validation\n",
      "Epoch: 147 \tTraining Loss: 0.268709 \tTraining acc: 0.89 \tValidation Loss: 0.799832 \tAccuracy: 0.74 \tF1-Score: 0.76\n",
      "validation\n",
      "Epoch: 148 \tTraining Loss: 0.264642 \tTraining acc: 0.89 \tValidation Loss: 0.786070 \tAccuracy: 0.75 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 149 \tTraining Loss: 0.276035 \tTraining acc: 0.89 \tValidation Loss: 0.800532 \tAccuracy: 0.73 \tF1-Score: 0.77\n",
      "validation\n",
      "Epoch: 150 \tTraining Loss: 0.265542 \tTraining acc: 0.89 \tValidation Loss: 0.791068 \tAccuracy: 0.74 \tF1-Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_epochs= 150\n",
    "train_on_gpu = True\n",
    "size = 112\n",
    "\n",
    "eval_preprocess = eval_preprocess(size)\n",
    "\n",
    "train_preprocess = train_preprocess(size)\n",
    "\n",
    "train_dataset = VideoFrameDataset(\n",
    "    root_path='transfer/train',\n",
    "    annotationfile_path='transfer/train/annotations.txt',\n",
    "    num_segments=16,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=train_preprocess,\n",
    "    test_mode=False\n",
    ")\n",
    "test_dataset = VideoFrameDataset(\n",
    "    root_path='transfer/test',\n",
    "    annotationfile_path='transfer/test/annotations.txt',\n",
    "    num_segments=1,\n",
    "    frames_per_segment=16,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=eval_preprocess,\n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,#2\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,#2\n",
    "    pin_memory=True\n",
    ")\n",
    "'''\n",
    "model = MobileNetV2(num_classes=6, sample_size=size, width_mult=1.)\n",
    "#model.load_state_dict(torch.load('pretrained/kinetics_mobilenetv2_1.0x_RGB_16_best.pth'))'''\n",
    "\n",
    "if train_on_gpu:\n",
    "    model = nn.DataParallel(model)\n",
    "    model.cuda()\n",
    "    print('train on gpu')\n",
    "else:\n",
    "    model.cpu()\n",
    "    print('train on cpu')\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min',patience=10,verbose=True,min_lr=1e-10)\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "epoch_train_loss =[]\n",
    "epoch_val_loss =[]\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    train_pred = np.array([],dtype='i')\n",
    "    train_truth = np.array([],dtype='i')\n",
    "    for data, target in train_dataloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        train_pred = np.concatenate((train_pred, np.argmax(output.clone().detach().cpu().numpy(),axis=1)))\n",
    "        train_truth = np.concatenate((train_truth, target.clone().detach().cpu().numpy()))\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    y_pred = np.array([],dtype='i')\n",
    "    y_truth = np.array([],dtype='i')\n",
    "    print('validation')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            y_pred = np.concatenate((y_pred, np.argmax(output.clone().detach().cpu().numpy(),axis=1)))\n",
    "            y_truth = np.concatenate((y_truth, target.clone().detach().cpu().numpy()))\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_dataloader.sampler)\n",
    "    valid_loss = valid_loss/len(test_dataloader.sampler)\n",
    "    epoch_train_loss.append(train_loss)\n",
    "    epoch_val_loss.append(valid_loss)\n",
    "    \n",
    "    trainacc=accuracy_score(train_truth, train_pred)\n",
    "    acc = accuracy_score(y_truth, y_pred)\n",
    "    rec = recall_score(y_truth, y_pred, average='macro')\n",
    "    prec = precision_score(y_truth, y_pred, average='macro')\n",
    "    f1 = f1_score(y_truth, y_pred, average='macro')\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining acc: {:.2f} \\tValidation Loss: {:.6f} \\tAccuracy: {:.2f} \\tF1-Score: {:.2f}'.format(\n",
    "    epoch, train_loss, trainacc, valid_loss, acc, f1))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.module.state_dict(), 'P3D63_Global_2.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "    \"\"\"\n",
    "    wandb.log({\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': valid_loss,\n",
    "        'accuracy': acc,\n",
    "        'f1-score': f1,\n",
    "        'recall': rec,\n",
    "        'precission': prec,\n",
    "    }, step=epoch) \n",
    "    \"\"\"\n",
    "    \n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efe73e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "17.536867초\n"
     ]
    }
   ],
   "source": [
    "test_dataset = VideoFrameDataset(\n",
    "    root_path='test/finalfull/test',\n",
    "    annotationfile_path='test/finalfull/test/annotations.txt',\n",
    "    num_segments=1,\n",
    "    frames_per_segment=16,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=eval_preprocess,\n",
    "    test_mode=False\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,#2\n",
    "    pin_memory=True\n",
    ")\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "model.eval()\n",
    "y_pred = np.array([],dtype='i')\n",
    "print('validation')\n",
    "with torch.no_grad():\n",
    "    for data, target in test_dataloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        y_pred = np.concatenate((y_pred, np.argmax(output.clone().detach().cpu().numpy(),axis=1)))\n",
    "terminate_time = timeit.default_timer()\n",
    "print(\"%f초\" % (terminate_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5940919a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 5 1 5 0 5 1 1 1 5 1 5 1 1 0 1 1 5 1 0 5 0 0 0 1 5 1 1 1 0 0 5 5 5 5 0 1\n",
      " 0 5 5 0 5 5 1 5 1 1 0 5 1 5 0 5 1 0 1 1 1 1 1 0 0 5 1 1 1 0 5 1 0 1 0 5 0\n",
      " 0 1 0 5 1 5 0 5 5 5 1 5 1 5 1 5 5 5 5 0 5 1 5 5 5 1 5 1 0 1 0 5 1 0 0 5 1\n",
      " 0 5 5 1 5 5 5 0 1 1 1 1 5 1 5 0 5 1 5 0 1 1 1 5 0 5 1 0 0 0 5 5 1 5 1 1 1\n",
      " 5 5 5 1 0 0 5 1 1 5 0 1 0 1 0 5 1 0 5 5 0 0 5 5 0 1 5 0 0 1 0 5 5 1 0 5 1\n",
      " 1 5 0 0 5 0 5 1 5 5 5 5 5 1 1 5 0 5 0 1 5 0 1 0 1 5 1 0 5 5 5 0 1 1 5 5 1\n",
      " 1 0 1 1 5 1 0 0 1 5 5 5 1 1 1 5 0 5 0 0 5 5]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a15d349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 5 1 1 1 1 2 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 5 1 1 1 5 1 1 5 1 1 1 1 1 5 1 1 1 1\n",
      " 1 1 1 1 1 1 5 1 1 1 1 1 1 1 5 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 5 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5\n",
      " 1 5 5 5 5 5 5 0 5 1 1 5 5 5 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 5 5 5 5 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as numpy\n",
    "y_truth=numpy.loadtxt('final/label.txt',dtype='int', delimiter=\"\\n\")\n",
    "print(y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bcc3b84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3237704918032787"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_truth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52f7e9ce",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x=y_pred\n",
    "i=0\n",
    "for a in y_pred:\n",
    "    if a == 1 or a == 5:\n",
    "        print(a)\n",
    "        x[i]=1\n",
    "    else :\n",
    "        x[i]=0\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53091eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1\n",
      " 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ff30bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y_truth\n",
    "i=0\n",
    "for a in y_truth:\n",
    "    if a == 1 or a == 5:\n",
    "        y[i]=True\n",
    "    else :\n",
    "        y[i]=False\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29909315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08594bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6311475409836066"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(x, y) #1과 5를 같은 행동이라고 생각했을때 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4e6992a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "3\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "3\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x=y_pred\n",
    "i=0\n",
    "for a in y_pred:\n",
    "    if a == 1 or a==3 or a == 5:\n",
    "        x[i]=1\n",
    "    else :\n",
    "        x[i]=0\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53cb0596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1\n",
      " 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1\n",
      " 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1\n",
      " 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8fedece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y_truth\n",
    "i=0\n",
    "for a in y_truth:\n",
    "    if a == 1 or a==3 or a == 5:\n",
    "        y[i]=True\n",
    "    else :\n",
    "        y[i]=False\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2726e70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09725528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6516393442622951"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(x, y) #집중 비집중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b7d995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch17_p38",
   "language": "python",
   "name": "pytorch17_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
